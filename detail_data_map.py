from camoufox.sync_api import Camoufox
from bs4 import BeautifulSoup
import time
import json
import os

# ------------------ Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ ------------------ #
def extract_links_from_google_maps(search_url):
    results = []

    with Camoufox(headless=False) as browser:
        page = browser.new_page()
        page.goto(search_url, wait_until="domcontentloaded", timeout=120000)
        page.wait_for_timeout(5000)

        scroll_box = page.query_selector('div.m6QErb.DxyBCb.kA9KIf.dS8AEf.ecceSd')

        seen_titles = set()
        counter = 1

        if scroll_box:
            box_bounds = scroll_box.bounding_box()
            if box_bounds:
                page.mouse.move(
                    box_bounds['x'] + box_bounds['width'] / 2,
                    box_bounds['y'] + box_bounds['height'] / 2
                )
                time.sleep(1)

                while True:
                    page.mouse.wheel(0, 800)
                    time.sleep(1.5)

                    titles = page.query_selector_all('span[dir="rtl"]')
                    links = page.query_selector_all('a.hfpxzc')

                    for t, l in zip(titles, links):
                        title = t.inner_text().strip()
                        href = l.get_attribute("href")
                        if title and title not in seen_titles:
                            seen_titles.add(title)
                            results.append({
                                "index": counter,
                                "title": title,
                                "link": href
                            })
                            counter += 1

                    # Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§ÛŒÙ†Ø¬Ø§ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªØ¹Ø¯Ø§Ø¯ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯
                    if counter > 10:  # Ù…Ø«Ù„Ø§ ÙÙ‚Ø· 10 Ù…Ú©Ø§Ù† Ø§ÙˆÙ„
                        break

    return results

# ------------------ Ù…Ø±Ø­Ù„Ù‡ Ø¯ÙˆÙ…: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù‡Ø± Ù„ÛŒÙ†Ú© ------------------ #
def extract_place_data_from_html(html_content, place_url=None):
    soup = BeautifulSoup(html_content, "html.parser")

    def get_text_or_none(selector):
        element = soup.select_one(selector)
        return element.get_text(strip=True) if element else None

    def get_attr_or_none(selector, attr):
        element = soup.select_one(selector)
        return element[attr] if element and element.has_attr(attr) else None

    data = {
        "title": get_text_or_none("h1.DUwDvf"),
        "subtitle": get_text_or_none("h2.bwoZTb"),
        "rating": get_text_or_none("span[aria-hidden='true']"),
        "reviews_count": get_text_or_none("button.GQjSyb span"),
        "category": get_text_or_none("button.DkEaL"),
        "address": get_text_or_none("button[data-item-id='address'] div.Io6YTe"),
        "phone": get_text_or_none("button[data-item-id^='phone:'] div.Io6YTe"),
        "opening_hours": [li.get_text(strip=True) for li in soup.select("table.eK4R0e li")] or None,
        "main_image": get_attr_or_none("div.RZ66Rb img", "src"),
        "location_coordinates": None
    }

    # Ø§Ú¯Ø± Ù…Ø®ØªØµØ§Øª Ø¯Ø± Ù„ÛŒÙ†Ú© Ø¨Ø§Ø´Ø¯
    if place_url and "@" in place_url:
        try:
            coords_part = place_url.split("@")[1].split(",")[:2]
            data["location_coordinates"] = {
                "lat": coords_part[0],
                "lng": coords_part[1]
            }
        except:
            pass

    return data

# ------------------ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø±Ù†Ø§Ù…Ù‡ ------------------ #
def main():
    search_url = "https://www.google.com/maps/search/Ù‚Ø²ÙˆÛŒÙ†+ØŒ+Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øªâ€¬â€¬/@36.2898665,49.9831087,13z"
    output_file = "places_info.txt"

    # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù‚Ø¨Ù„ÛŒ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
    if os.path.exists(output_file):
        os.remove(output_file)

    print("ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ø² Ù†Ù‚Ø´Ù‡ Ú¯ÙˆÚ¯Ù„...")
    places = extract_links_from_google_maps(search_url)

    print(f"âœ… {len(places)} Ù…Ú©Ø§Ù† Ù¾ÛŒØ¯Ø§ Ø´Ø¯. Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¬Ø²Ø¦ÛŒ...")

    with Camoufox(headless=True) as browser:
        for place in places:
            try:
                page = browser.new_page()
                page.goto(place["link"], wait_until="domcontentloaded", timeout=60000)
                page.wait_for_timeout(4000)

                html = page.content()
                details = extract_place_data_from_html(html, place["link"])

                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ
                with open(output_file, "a", encoding="utf-8") as f:
                    f.write(f"{place['index']}. Ø¹Ù†ÙˆØ§Ù†: {place['title']}\nÙ„ÛŒÙ†Ú©: {place['link']}\n")
                    for k, v in details.items():
                        f.write(f"{k}: {v}\n")
                    f.write("-" * 60 + "\n")

                print(f"âœ… {place['index']}. {place['title']} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù…Ú©Ø§Ù† {place['index']}: {e}")

    print(f"\nğŸ“ ØªÙ…Ø§Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø± ÙØ§ÛŒÙ„ `{output_file}` Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

# ------------------ Ø§Ø¬Ø±Ø§ ------------------ #
if __name__ == "__main__":
    main()
